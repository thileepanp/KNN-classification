{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In this practice I'm trying to learn classification\n",
    "#I will be using the K nearest neighbour algorithm for\n",
    "#classification\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP1: UNDERSTANDING THE DATA\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP1: UNDERSTANDING THE DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys of iris_dataset: \n",
      "dict_keys(['target_names', 'target', 'DESCR', 'feature_names', 'data'])\n"
     ]
    }
   ],
   "source": [
    "#using the iris dataset for practice\n",
    "from sklearn.datasets import load_iris \n",
    "\n",
    "#Load and return the iris dataset\n",
    "iris_dataset=load_iris() \n",
    "\n",
    "#Let's Print the keys of the dataset in the given format\n",
    "print(\"keys of iris_dataset: \\n{}\".format(iris_dataset.keys())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive att\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The 'DESCR' key in the dataset is a short description of the dataset.\n",
    "print(iris_dataset['DESCR'][:193] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target names: \n",
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "#let's print the names of the target classes\n",
    "print(\"target names: \\n{}\". format(iris_dataset['target_names'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: \n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "#Let's print the names of the features\n",
    "print(\"Feature names: \\n{}\".format(iris_dataset['feature_names']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of data: \n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#The data itself is contained in the 'target' and the 'data' fields\n",
    "print(\"Type of data: \\n{}\".format(type(iris_dataset['data'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This shows the data is stored as Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: \n",
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "#Let's see the shape of the 'data' field\n",
    "print(\"Shape of data: \\n{}\".format(iris_dataset['data'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#So, the data is stored in 150 rows and 4 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's print the first 5 rows of the data field\n",
    "iris_dataset['data'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Each of these 4 columns correspond to the 4 features as printed earlier\n",
    "# ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of target: \n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Let's now take a look at the 'target' field\n",
    "print(\"type of target: \\n{}\".format(type(iris_dataset['target'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of target field: \n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "#Shape of the target variable\n",
    "print(\"shape of target field: \\n{}\".format(iris_dataset['target'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#So the target variable is a single array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Target: \\n{}\".format(iris_dataset['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#So the 3 target classes are encoded as binary variables from 0-2\n",
    "#each variable corresponds to ['setosa' 'versicolor' 'virginica']\n",
    "#respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP2: SPLITTING THE DATA\n"
     ]
    }
   ],
   "source": [
    "print(\"STEP2: SPLITTING THE DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We need 'new' data to test our model after training it with data\n",
    "#This is needed because after we build a model, we train it with \n",
    "#data, so the model can learn from the data and predict future\n",
    "#inputs. \n",
    "\n",
    "#While training, the models 'gets used' to the data and 'remembers'\n",
    "#it. So, if we again use the same data and ask the model to predict, \n",
    "#we can't judge the accuracy of the model since it has already \n",
    "#learned from the same data. \n",
    "\n",
    "#For this purpose, we split the data into two parts and use one part\n",
    "# to train the data and the other to test it. \n",
    "\n",
    "# The data used to train is called the 'Training data' or 'Training set'\n",
    "# and the testing data is called the 'Test data' or 'Test set'. \n",
    "\n",
    "# Using a 75% - 25% split i.e. using 75% of the data as training data \n",
    "# and 25% of the data as testing data is a good rule of thumb\n",
    "\n",
    "#sklearn already has inbuilt functions to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" The train test split function shuffles the data randomly before \n",
    "splitting. This is done by the 'random_state' parameter in the function. \n",
    "\n",
    "More information about the function is given below [from Python help]\n",
    "\n",
    "Signature: train_test_split(*arrays, **options)\n",
    "Docstring:\n",
    "Split arrays or matrices into random train and test subsets\n",
    "\n",
    "Quick utility that wraps input validation and\n",
    "``next(ShuffleSplit().split(X, y))`` and application to input data\n",
    "into a single call for splitting (and optionally subsampling) data in a\n",
    "oneliner.\n",
    "\n",
    "Read more in the :ref:`User Guide <cross_validation>`.\n",
    "\n",
    "Parameters\n",
    "----------\n",
    "*arrays : sequence of indexables with same length / shape[0]\n",
    "    Allowed inputs are lists, numpy arrays, scipy-sparse\n",
    "    matrices or pandas dataframes.\n",
    "\n",
    "test_size : float, int, or None (default is None)\n",
    "    If float, should be between 0.0 and 1.0 and represent the\n",
    "    proportion of the dataset to include in the test split. If\n",
    "    int, represents the absolute number of test samples. If None,\n",
    "    the value is automatically set to the complement of the train size.\n",
    "    If train size is also None, test size is set to 0.25.\n",
    "\n",
    "train_size : float, int, or None (default is None)\n",
    "    If float, should be between 0.0 and 1.0 and represent the\n",
    "    proportion of the dataset to include in the train split. If\n",
    "    int, represents the absolute number of train samples. If None,\n",
    "    the value is automatically set to the complement of the test size.\n",
    "    random_state : int or RandomState\n",
    "    Pseudo-random number generator state used for random sampling.\n",
    "\n",
    "stratify : array-like or None (default is None)\n",
    "    If not None, data is split in a stratified fashion, using this as\n",
    "    the class labels.\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_dataset['data'],\n",
    "                                iris_dataset['target'], random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (112, 4)\n",
      "y_train shape: (112,)\n"
     ]
    }
   ],
   "source": [
    "#Let's see if training contain 75% of data \n",
    "print(\"X_train shape: {}\".format( X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (38, 4)\n",
      "y_test shape: (38,)\n"
     ]
    }
   ],
   "source": [
    "#Let's check if the test data contains 25% of data\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_test shape: {}\". format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
